{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abca79b5",
   "metadata": {},
   "source": [
    "# Introduction to LLMs with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaef042",
   "metadata": {},
   "source": [
    "<img src=\"https://heidloff.net/assets/img/2023/02/transformers.png\">\n",
    "\n",
    "Everything started with the Transformer architecture, introduced in the paper “Attention Is All You Need”. BERT (Bidirectional Encoder Representations from Transformers) was one of the first major models built on top of this architecture and played a key role in advancing natural language understanding. Modern generative AI models such as ChatGPT, Gemma, and Claude are also based on the Transformer architecture.\n",
    "\n",
    "The main difference between Transformers and older text generation models lies in their ability to understand the context of each token in a sentence. While traditional RNN-based models tend to lose contextual information in long sequences, Transformers can maintain long-range dependencies, even in large textual contents such as financial documents, web pages, or books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72ec1b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc978617",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2f7325",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb6efa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4786c6cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ca48fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c4fe0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
